Study
=====

My study notes will draw heavily from the required texts and multimedia. I will
also draw from external sources that I find to be adept at explaining a
particular topic. If something is referenced here, it is because I found it to
be very useful in understanding a topic.

# Standard Mathematical and Statistical Notation
Notes below are from the following sources; [@bhattiprelim].

## Vector and Matrix Notation

 - A _scalar_ is a number. _Scalars_ are represented by lower case letters from the beginning of the alphabet such as $a, b, c$ etc.

 - A _vector_ is a $n \times 1$ array defined with the mathematical operations of addition and multiplication. The standard convention is for all vectors to be column vectors, i.e. they are 'long' with $n$ rows and $1$ column. Vectors are represented as __bold__ faced lower case letters frequently from the end of the alphabet, such as $\mathbf{x}, \mathbf{u},$ and $\mathbf{v}$. the $i$th entry of a vector $\mathbf{u}$ is denoted by $\mathbf{u}[i]=u_i$.

 - A _matrix_ is a $n \times m$ array defined with the mathematical operations of addition and multiplication. Matricies are represented by a bold face upper cased letter such as $\mathbf{A}, \mathbf{W}, \mathbf{X}$, etc. The $(i,j)$th entry of a matrix $\mathbf{A}$ is denoted by $\mathbf{A}[i,j] = a_{ij}$.

 - The transpose of a $(n \times 1)$ column vector $\mathbf{a}$ is the $(1 \times n)$ row vector $\mathbf{a}^T = [a_1 \ldots a_n]$. Sometimes the transpose $\mathbf{a}^T$ is denoted by $\mathbf{a}^\prime$.

 - The transpose of a $(n \times m)$ matrix $\mathbf{A}$ is the $(m \times n)$ matrix $\mathbf{A}^T$ where $\mathbf{A}[i,j] = \mathbf{A}^T[j,i]$. When a matrix is transposed, the rows become the columns and the columns become the rows.

 - It is preferred to use the $T$ notation $\mathbf{a}^T$ instead of the "prime notation" $\mathbf{a}^\prime$.

## Random Variable Notation
Random variables are how you develop calculus based probability theorey.Random variables are the unkown statistical experiement that generate data. Statistical theory is based upon the concept of random sampling.

 - Random variables are denoted by capital letters from the end of the alphabet such as $U$, $V$, $X$, $Y$, or $Z$.

 - The _observed value_ of a random variable is denoted by the lower cased counterpart such as $u$, $v$, $x$, $y$, or $z$.

 - When we have a _random sample_ of independent and identically distributed (iid) random variables, we will index the variables in a set such as ${X_1, X_2, \ldots, X_n}$ for the random variables and ${x_1, x_2, \ldots, x_n}$ for the observed values.

- Random variables are used to devleop statistical estimators. Observes values of random variables are used to compute statistical estimates.

- Random variable notation can become convoluded when we move to multivariate random variables. Pay attention to how an author presents these concepts in text.

## 'Distribution'
The term _distribution_ is used throughout all statistical applications and discussions. Loosly speaking, the term distribution is meant to describe how a group of values are related to either each other or to the range of values on which they are defined (their _support_).

The term _distribution_ is used rather sloppily. If you don't understand the context you wont understand the use. The term _distribution_ is mapped to many related concepts. In general the term _distribution_ is related to the characterization of a random variable, or data generated by a random variable.

There are many mathematical notations for characterizing a statistical distribution. The choice of characterization will depend on the context and the existence of the characterization. A random variable can be characterized by any of the following functions.

 - The _cumulative distribution function_ (cdf), denoted by $F(x) = Pr(X \leq x)$. the cdf will exist for all random variables, and in general is why we use the term "distribution" so looslely throughout statistics. cdf exists for all random variables. From data you can always estimate a distribution function.

 - The _probability density function_ (pdf) for continuous random variables, denoted by $f(x)$, or the _probability mass function_ (pmf) for discrete random variables, denoted by $p(x)$. Note that neither of these functions are guaranteed to exist. A random variable that can be described with a cdf will not always possess a pdf or pmf.

 - Transformation functions such as the moment generating function $m(t) = \mathbb{E}[\exp(tX)]$ and the characteristic function $\phi(t) = \mathbb{E}[\exp(itX)]$. Transformation functions are not used when working with data, they may be used to develop conceptual underpinnings of modeling.

 - Specialized representations for particular applications such as the _hazard fucntion_ $h(t) = \frac{f(t)}{S(t)}$ and the _survival function_ $S(t) = 1 - F(t)$ used in Survival Analysis. Survival function is a simple map of the cdf. The hazard function allows you to get a generic representation of a survival function.

 - In data analysis distributions can be analyzed using the empirical cdf, the histogram, the Quantile-Quantile plot, and the Kolmogorov-Smirnov test.

 - If you need to assess the distribution of residuals in linear regression and compare that to the assumption that they are normally distributed.

## Mathematical Expectation
Mathematical Expertation is the theoretical averaging of a random variable with respect to its distribution function. In this sense the pdf of pmf act as a weight function that allows you to find the "center" of the distribution.

For a continuous random variable $X$ with pdf function $f(x)$, the mathematical expectation of $X$ can be computed by

$$\mathbb{E}[X] = \int xf(x)dx$$

For a discrete random variable $X$ with pmf function $p(x) = Pr(X=x)$, the mathematical expectation of $X$ can be computed by

$$\mathbb{E}[X] = \sum_{x}xp(x)$$

$\mathbb{E}[X]$ is also referred to as the first moment of X.

## Expectation, Variance, and Covariance as Mathematical Operators

Let $X$ denote a random variable. Consider the affine transformation $aX+b$.

 - $\mathbb{E}[aX+b] = a\mathbb{E}[X] + b$
 - $\mathrm{Var}[aX+b] = a^2\mathrm{Var}[X]$

Let $X$ and $Y$ be random variables with a joint distribution function. (In the continuous case we would denote this joint distribution function by the join density function $f(x,y)$.) Consider the linera transformtions $aX$ and $bY$.

 - $\mathbb{E}[aX+bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$
 - $\mathrm{Var}[aX+bY] = a^2\mathrm{Var}[X] + b^2\mathrm{Var}[Y] + ab\mathrm{Cov}[X,Y]$

Here the reader should note that in general $\mathrm{Cov}[aX+b,cY+d] = ac\mathrm{Cov}[X,Y]$. If $X$ and $Y$ are independent random variables, then $\mathrm{Cov}[X,Y] = 0$. The converse of this statement is not true except when both $X$ and $Y$ are normally distributed. In general $\mathrm{Cov}[X,Y] = 0$ does not imply that $X$ and $Y$ are indepdented random variables.

\newpage

# Statistical Assumptions for Ordinary Least Squares Regression
Notes below are from the following sources; [@bhattiolsassums].

 - In Ordinary Lease Squares (OLS) regression we wish to model a continuous random variable $Y$ (the response variable) given a set of _predictor variables_ $X_1, X_2, \ldots, X_k$.

 - While we require that the response variable $Y$ will be continuous, or approximately continuous. The _predictor variables_ $X_1, X_2, \ldots, X_k$ can be either continuous or discrete.

 - It is fairly standard notation to reserve $k$ for the number of predictor variables in the regression model, and $p$ for the number of parameters (regression coefficients or $\beta$s).

 - When formulating a regression model, we want to explain the variation in the response variable by the variation in the predictor variable.

## Statistical Assumptions for OLS Regression

There are two primary assumptions for OLS regression:

1. The regression model can be expressed in the form $$Y = \beta_0 + \beta_1X_1 + \ldots + \beta_kX_k + \epsilon$$ Notice that the model formulation specifies error term $\epsilon$ to be additive, and that the model parameters ($\beta$s) enter the modeling linearly, that is, $\beta_i$ represents the change in $Y$ for a one unit increase in $X_i$ when $X_i$ is a continuous predictor variable. Any statistical model in which the parameters enter the model linearly is referred to as a _linear model_.

2. The response variable $Y$ is assumed to come from an independent and identically distributed (iid) random sample from a $N(\mathbf{X\beta},\sigma^2)$ distribution where the variance of $\sigma^2$ is a fixed but unkown quantity. The statistical notation for this assumption is $Y ~ N(\mathbf{X\beta},\sigma^2)$.

## Linear Versus Nonlinear Regression

Remember that a _linear model_ is linear in the parameters, not the predictor variables.

 - The following regression models are all linear regression models: $$Y = \beta_0 + \beta_1X_1+\beta_2X_1^2 + \epsilon$$ $$Y = \beta_0 \beta_1\ln(X_1) + \epsilon$$

 - The following regression models are all nonlinear regression models: $$Y = \beta_0\exp(\beta_1X_1) + \epsilon$$ $$Y = \beta_0 + \beta_2\sin(\beta_1X_1) + \epsilon$$

 - If you know a little calculus, then there is an easy mathematical definition of a nonlinear regression model. In a nonlinear regression model at least one of the partial derivatives will be dependent on a model parameter.

 - Any quantity that has a $\beta$ in front of it counts as a degree of freedom used, and subsequently counts as a predictor variable.

 - A hint to identify a nonlinear model is when a parameter is within a function, specifically a nonlinear function.

## Distributional Assumptions for OLS Regression

The assumption $Y ~ N(\mathbf{X\beta},\sigma^2)$ can also be presented in terms of the error term $\epsilon$. Most introductory bookos present the distributional assumption in terms of the error term $\epsilon$, but more advanced books will use the standard Generalized Linear Model (GLM) presentation in terms of the response variable $Y$.

In terms of the error term $\epsilon$ the distributional assumption can also be presented as:

 - The error term $\epsilon ~ N(0,\sigma^2)$. Since $Y ~ N(\mathbf{X\beta},\sigma^2)$, then $\epsilon = Y - \mathbf{X\beta}$ has a $N(0,\sigma^2)$.

## Distributional Assumptions in Terms of the Error

1. The errors are normally distributed.
2. The errors are mean zero.
3. The errors are independent and identically distributed (iid).
4. The errors are _homoscedastic_, i.e. the errors do not have any correlation "in time or space".

When we build statistical models, we will check the assumptions about the errors by assessing the model _residuals_, which are our estimates of the error term.

_Homoscedasticity_: a sequence or vector of random variables is _homoscedastic_ if all random variables in the sequence or vector have the same finite variance. This is also known as the _homogeneity of variance_. [@wiki:homoscedasticity]

You'd need a pretty gross violation of _homoscedastic_ in the kind of problems that we work with today.

## Further Notation and Details

When we estimate an OLS regression model, we will be working with a random sample of response variables $Y_1, Y_2, \ldots, Y_n$, each with a vector of predictor variables $[X_{1i}, X_{2i},\ldots,X_{ki}]$. In matrix notation we will denote the regression problem by $$Y_{(n \times 1)} - X_{(n \times p)}\beta_{(p \times 1)} + \epsilon_{(n \times 1)}$$ where the matrix size is denoted by the subscript. Note that $X = [1, X_1, X_2, \ldots, X_k]$ and $\beta = [\beta_0, \beta_1, \beta_2, \ldots, \beta_k]$.

 - When we want to express the regression in terms of a single observation, the new typically use the $i$ subscript notation $$Y_i = \mathbf{X_i\beta} + \epsilon_i$$ or simply $$Y_i = \beta_0 + \beta_1X_{1i} + \ldots + \beta_kX_{ki} + \epsilon_i$$

\newpage

# Estimation and Inference for Ordinary Least Squares Regression
Notes below are from the following sources; [@bhattiestimols].

It's important to understand some aspects of estimation and inference for every statistical method that is used.

## Estimation - Simple Linear Regression

 - A _simple linear regression_ is the special case of an OLS regression model with a single predictor variable. $$Y = \beta_0 + \beta1X + \epsilon$$

 - For the $i$th observation we will denote the regression model by $$Y_i = \beta_0 + \beta_1X_i + \epsilon_i$$

 - For the random sample $Y_1, Y_2, \ldots, Y_n$ we can estimate the parameters $\beta_0$ and $\beta_1$ by minimizing the sum of the squared errors, $$\min\sum_{i=1}^{n}\epsilon_i^2$$ which is equivalent to minimizing $$\min\sum_{i=1}^{n}(Y_i - \beta_0 - \beta_1X_i)^2$$

## Estimators and Estimates for Simple Linear Regression

 - The estimators for $\beta_0$ and $\beta_1$ can be computed analytically and are given by $$\hat{\beta_1} = \frac{\sum(Y_i - \bar{Y})(X_i - \bar{X})}{(X_i - \bar{X})^2} = \frac{\mathrm{Cov}(Y,X)}{\mathrm{Var}(X)}$$ and $$\hat{\beta_0} = \bar{Y} - \hat{\beta_1}\bar{X}$$

 - The regression line always goes through the centroid $(\bar{X},\bar{Y})$.

 - We refer to the formulas for $\hat{\beta_0}$ and $\hat{\beta_1}$ as estimators and the values that these formulas can take for a given random sample as the estimates.

 - In statistics we put hats on all estimators and estimates.

 - Given $\hat{\beta_0}$ and $\hat{\beta_1}$ the predicted value or fitted value is given by $$\hat{Y} = \hat{\beta_0} + \hat{\beta_1}X$$

## Estimation - The General Case

 - We seldom build regression models with a single predictor variable. Typically we have multiple predictor variables denoted by $X_1, X_2, \ldots, X_k$, and hence the standard regression case is sometimes referred to as _multiple regression_ in introductory regression texts.

 - We can still think about the estimation of $\beta_0, \beta_1, \beta_2, \ldots, \beta_k$ in the same manner as the sime linear regression case $$\min\sum_{i=1}^n(Y_i - \beta_0 - \beta_1X_{1i} - \beta_2X_{2i} - \ldots - \beta_kX_{ki})^2$$ but the computations will be performed as matrix computations.

## General Estimation - Matrix Notation

Before we set up the matrix formulation for the OLS model, let's begin by defining some matrix notation.

 - The error vector $\epsilon = [\epsilon_1, \ldots, \epsilon_n]^T$.
 - The response vector $Y = [Y_1, \ldots, Y_n]^T$.
 - The design matrix or predictor matrix $X = [1, X_1, X_2, \ldots, X_k]$.
 - The parameter vector $\beta = [\beta_0, \beta_1, \beta_2, \ldots, \beta_k]^T$.

 - All vectors are column vectors, and the superscript $T$ denotes the vector or matrix _transpose_.

## General Estimation - Matrix Computations

 - We minimize the sum of the squared error by minimizing $S(\beta) = \epsilon^T\epsilon$ which can be re-expressed as $$S(\beta) = (Y - X\beta)^T(Y - X\beta)$$

 - Taking the matrix derivative of $S(\beta)$, we get $$S_\beta(\hat{\beta}) = -2X^TY + 2X^TX\hat{\beta}$$

 - Setting the matrix derivative to zero, we can write the expression for the least squares _normal equations_ $$X^TX\hat{\beta} = X^TY$$, which yield the estimator $$\hat{\beta} = (X^TX)^{-1}X^TY$$

 - The estimator form $\hat{\beta} = (X^TX)^{-1}X^TY$ assumes that the inverse matrix $(X^TX)^{-1}$ exists and can be computed. In practice your statistical software will directly solve the normal equations using a QR Factorization.

_normal equations_: projection of a linear space into a subspace to ensure that a solution exists.

_QR Factorization_: or QR decompositionof a matrix is a decomposition of a matrix $A$ into a product $A = QR$ of an orthogonal matrix $Q$ and an upper triangular matrix $R$ [@wiki:qrdecomposition].

## Statistical Inference with the t-Test

 - In OLS regression the statistical inference for the individual regression coefficients can be performed using a t-test.

_t-test_: any statistical test using a t-statistic to derive the test and the p-value for the test. Alternatively, any statistical test that uses a t-statistic as the decision variable.

_statistical test_: have a null and alternative hypothesis, and a test statistic with a known distribution.

 - When performing a t-test there are three primary components: (1) stating the null and alternative hypotheses, (2) Computing the value of the test statistic, and (3) deriving a statistical conclusion based on a desired significance level.

 - Step 1: The null and alternate hypotheses for $\beta_i$ are given by $$H_0:\beta_i = 0 \text{ versus } H_1:\beta_i \neq 0$$

 - Step 2: The t statistic for $\beta_i$ is computed by $$t_i = \frac{\hat{\beta_i}}{SE(\hat{\beta_i})}$$ and has a degrees of freedom equal to the sample size minus the number of model parameters, i.e. $df = n-dim(model)$. For example if you had a regression model with two predictor variables and an intercept estimated on a sample of size 50, then the t statistic would have 47 degrees of freedom.

 - Step 3: Reject the $H_0$ or Fail to Reject $H_0$ based on the value of your t statistic and your significance level. This decision can be made by using the p-value of your t statistic or by using the critical value for your significance level.

## Confidence Intervals for Parameter Estimates

An alternative to performing a formal hypothesis test is to use a confidence interval for your parameter estimate. There is a duality between confidence intervals and formal hypothesis testing for regression parameters.

 - The confidence interval for $\hat{\beta_i}$ is given by $$\hat{\beta_i} \pm t(df,\frac{\alpha}{2}) \times SE(\hat{\beta_i})$$ where $t(df,\frac{\alpha}{2})$ is a t value from a theoretical t distribution, not a t statistic value.

 - If the confidence interval does not contain zero, then this is the equivalent to rejecting the null hypothesis $H_0:\beta_i = 0$.

## Statistical Intervals for Predicted Values

The phrase _predicted value_ is used in statistics to refer to the in-sample _fitted values_ from the estimated model or to refer to the out-of-sample _forecasted values_. The dual use of this phrase can be confusing. A better habit is to use the phrase _in-sample fitted values_ and in the _out-of-sample predicted values_ to clearly reference these different values.

_inference_ is an in-sample activity, measuring the quality of the model based on in-sample performance. _predictive modeling_ is an out-of-sample activity, measuring the quality of the model based on out-of-sample.

 - Given $\hat{\beta} = (X^TX)^{-1}X^TY$ the vector of fitted values can be computed by $\hat{y} = X\hat{\beta} = HY$, where $H = X(X^TX)^{-1}X^T$. The matrix $H$ is called the _hat matrix_ since it puts the hat on $Y$.

 - The point estimate $\hat{Y_0}$ at the point $x_0$ can be computed by $\hat{Y_0} = x_0^T\hat{\beta}$.

 - The confidence interval for an in-sample point $x_0$ on the estimated regression function is given by $$x_0^T\hat{\beta} \pm \hat{\sigma} \sqrt{x_0^T(X^TX)^{-1}x_0}$$

 - The prediction interval for the point estimator $\hat{Y_0}$ for an out-of-sample $x_0$ is given by $$x_0^T\hat{\beta} \pm \hat{\sigma} \sqrt{1 + x_0^T(X^TX)^{-1}x_0}$$

 - Note that the out-of-sample prediction interval is always wider than the in-sample confidence interval.

## Further Notation and Details

In order to compute the t statistic you need the standard error of the parameter estimate. Most statistical software packages should provide this estimate and compute this t statistic for you However, it is always a good idea to know from where this number comes. Here are the details needed to compute the standard error for $\hat{\beta_i}$.

 - The estimated parameter vector $\hat{\beta}$ has the covariance matrix given by $$\mathrm{Cov}(\hat{\beta}) = \hat{\sigma}^2X^TX$$ where $$\hat{\sigma}^2 = \frac{SSE}{n - k - 1}$$

 - The variance of $\hat{\beta_i}$ is the $i$th diagonal element of the covariance matrix $$\mathrm{Var}(\hat{\beta_i}) = \hat{\sigma}^2(X^TX)_{ii}$$

\newpage

# Study Questions for Ordinary Least Squares Regression

__Question__: When we refer to a 'simple linear regression', to what type of
model are we referring? How does a 'simple linear regression' differ from a
'multiple regression'?

Response from [@montgomery2012introduction] pages 2 and 4.

$$ y = \beta_0 + \beta_1x + \varepsilon $$

The equation abovce is called a __linear regression model__. Customarily $x$ is
called the independent variable and $y$ is called the dependent variable.
However, this often causes confustion with the concept of statistical
independence, so we refer to $x$ as the __predictor__ or __regressor__and $y$ as
the __response__ variable. Because the equation above involves only one
__regressor__ variable, it is called a __simple linear regression model__.

In general the response variable $y$ may be related to $k$ regressors, $x_1,
x_2, \ldots, x_k$, so that:

$$ y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_kx_x + \varepsilon $$

This is called a __multiple linear regression model__ because more than one
regressor is involved. The adjetive linear is employed to indicate that the
model is linear in the parameters $\beta_0, \beta_1, \ldots, \beta_k$, not
because the $y$ is a linear function of the $x$'s.

__Question__: In statistics, and in this course, we use the term 'regression' as
a general term. What do we mean by the term 'regression'? What is the objective
of a 'regression model'?

Response from [@montgomery2012introduction] pages 1.

Regression analysis is a _statistical technique__ for investigating and
__modeling the relationship between variables__. The goal of regression analysis
is to determine the values of parameters for a function that causes the function
to best fit a set of data. Regression analysis helps one understand how the
typical value of the dependent variable changes when any one of the independent
variables is varied, while the other independent variables are held fixed
[@wiki:regressionanalysis].

__Question__: What do we mean by 'linear regression'? represent a linear
regression? Which equations represent a linear regression?

(a) $y = \beta_0 + \beta_1x_1$
(b) $y = \beta_0 + \beta_1x_1 + \beta_2x_2^2$
(c) $y = \beta_0 + \exp(\beta_1)x_1$

Both (a) and (c) are representations of linear regression because they only have
a single __regressor__. (b), with the presence of $x_2^2$ has two
__regressors__.

__Question__: Before building statistical models, it is a common and preferred
practice to perform an Exploratory Data Analysis (EDA). What constitutes an EDA
for a simple linear regression model? Is this EDA satisfactory for a multiple
regression model, or do we need to change or extend the EDA? As we move forward
in this course we will also learn about logistic regression models and survival
regression models, will these methods need their own EDA or is EDA general to
all statistical models?

__Question__: In the simple linear regression model what is the relationship
between R-squared and the correlation coefficient rho?

__Question__: How do we interpret a regression coefficient in OLS regression?

__Question__: Frequently, as a form of EDA for OLS regression we make a
scatterplot between the response variable Y and a predictor variable X. As an
assumption of OLS, the response variable Y must be continuous. However, the
predictor variable X could be continuous or discrete. When the predictor
variable is discrete, does a scatterplot still make sense? If not, what type of
visual EDA does make sense? Does the appropriateness of the scatterplot make
sense if the discrete variable takes on many discrete values (such as the set of
integers, think of dollar amounts rounded to the nearest dollar) versus only a
few discrete values(such as a coded categorical variable which only takes the
values 1, 2, or 3)?

__Question__: The simple linear regression model is a special case of 'Multiple
Regression' or 'Ordinary Least Squares'(OLS) regression. (We will typically use
the term OLS regression.) What are the assumptions of OLS regression? In the
final step of a regression analysis we perform a 'check of model adequacy'. What
model diagnostics do we use to validate our fitted model against the model
assumptions of OLS regression?

__Question__: How are the parameters, i.e. the model coefficients, estimated in
OLS regression? How does this relate to maximum likelihood estimation? How do
you show the relationship between OLS regression and maximum likelihood
estimation?

__Question__: What is the overall F-test? What is the null hypothesis and what
is the alternate hypothesis? The overall F-test is also called the 'test for a
regression effect'. Why is it called this?

__Question__: What is the difference between R-squared and adjusted R-squared?
How is each measure computed, and which measure should we prefer? How does the
interpretation of R-squared change as we move from the simple linear regression
model to the multiple regression model?

__Question__: The simple linear regression model $Y = b_0 + b_1*X_1$ has three
parameters. Two of the parameters are $b_0$ and $b_1$. What is the third
parameter?

__Question__: What is a sampling distribution? What theoretical distribution do
the parameter estimates have in OLS regression? What distribution do we use in
practice? Why do we use a different distribution in practice?

__Question__: The final step of a regression analysis is a 'check of model
adequacy'. This 'check of model adequacy' or 'goodness-of-fit' is a very
important step in regression analysis. Why? Which quantities in the regression
output are affected when the fitted model deviates from the underlying
assumptions of OLS regression?

__Question__: Nested Models: Given two regression models M1 and M2, what does it
mean when we say that 'M2 nests M1'?

__Question__: What is the Analysis of Variance Table for a regression model? How
do we interpret it and what statistical tests and quantities can be computed
from it?

__Question__: When the intercept is excluded in a regression model, how does the
computation and the interpretation of R-squared change? Fit a no intercept model
in SAS and check the SAS output for any noted differences.

__Question__: How do we interpret the diagnostic plots output by the
PLOTS(ONLY)=(DIAGNOSTICS) option in PROC REG in SAS?

__Question__: Why do we plot each predictor variable against the residual as a
model diagnostic?

__Question__: Why do we perform transformations in the construction of
regression models? Name at least two reasons.

__Question__: What is multicollinearity and how does it affect the parameter
estimates in OLS regression? How do we diagnose multicollinearity?

__Question__: What is a Variance Inflation Factor (VIF) and how does it relate
to multicollinearity?

__Question__: Given a set of predictor variables $X_1,\ldots, X_n$, which are
determined to show a high degree of multicollinearity between some of the
variables, how should we choose a subset of these predictor variables to reduce
the degree of multicollinearity and improve our OLS regression performance?

__Question__: Variable Selection: How does forward variable selection work? How
does backward variable selection work? How does stepwise variable selection
work?

\newpage

# Study Questions for Multivariate Analysis

## Principle Components Analysis

__Question__: Principal Components Analysis (PCA): What is principal components
analysis? How does PCA eliminate the problem of multicollinearity? What does it
mean for X1 and X2 to be orthogonal? In order to better understand
orthogonality, take the building prices data set and perform these steps:

(a) Perform a PROC CORR on X1-X9.
(b) Create nine orthogonal predictor variables using PCA. Call
these variables Z1-Z9.
(c) Perform a PROC CORR on Z1-Z9.

__Question__: Principal Components Analysis is described as a method of
'dimension reduction'. How does PCA reduce the dimension of a statistical
problem? How do you select the reduced dimension for your problem.

## Factor Analysis

__Question__: Are the factor scores always orthogonal? Are they orthogonal after
a rotation?

__Question__: If two analysts perform a factor analysis, are they likely to
arrive at the same result? If the same two analysts perform a principal
components analysis, are they likely to get the same result?

__Question__: What is the first step in performing a factor analysis?

__Question__: In the context of factor analysis, what is the communality of
factors?

## Cluster Analysis

__Question__: What is the difference between hierarchical and non-hierarchical
clustering?

__Question__: What is linkage? What types of linkage are there?

__Question__: How do we examine the goodness-of-fit of a cluster analysis or two
comparative cluster analyses?

__Question__: Do the data need to be treated before we perform a cluster
analysis?

\newpage

# References